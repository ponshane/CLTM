{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "# set python syspath to point out location of our self-writing module\n",
    "sys.path.append(\"/home/ponshane/work_dir/CLTM/src/codebase/\")\n",
    "\n",
    "from fasttext import FastVector\n",
    "\n",
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)\n",
    "\n",
    "def load_bilingual_dict(dictionary_path):\n",
    "    bilingual_dict = []\n",
    "    with open(dictionary_path, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            temp = line.strip(\"\\n\").split(\" \")\n",
    "            bilingual_dict.append((temp[0], temp[1]))\n",
    "        return bilingual_dict\n",
    "\n",
    "def export_to_required_input_of_CLTM(en_dictionary, zh_dictionary, outpath):\n",
    "    fout = open(outpath, \"w\")\n",
    "    for token in en_dictionary.id2word:\n",
    "        vector_components = [\"%.6f\" % number for number in en_dictionary[token]]\n",
    "        vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "        out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "        fout.write(out_line)\n",
    "\n",
    "    for token in zh_dictionary.id2word:\n",
    "        vector_components = [\"%.6f\" % number for number in zh_dictionary[token]]\n",
    "        vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "        out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "        fout.write(out_line)\n",
    "    fout.close()\n",
    "    \n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align two word space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from /home/ponshane/Downloads/cc.en.300.vec\n",
      "reading word vectors from /home/ponshane/Downloads/cc.zh.300.vec\n"
     ]
    }
   ],
   "source": [
    "en_dictionary = FastVector(vector_file='/home/ponshane/Downloads/cc.en.300.vec', max_vocab_size=100000)\n",
    "zh_dictionary = FastVector(vector_file='/home/ponshane/Downloads/cc.zh.300.vec', max_vocab_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028019036355238686\n"
     ]
    }
   ],
   "source": [
    "print(FastVector.cosine_similarity(en_dictionary[\"divide\"], zh_dictionary[\"分裂\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilingual_dict = load_bilingual_dict(dictionary_path=\"/home/ponshane/Downloads/en-zh.0-5000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form the training matrices\n",
    "source_matrix, target_matrix = make_training_matrices(\n",
    "    en_dictionary, zh_dictionary, bilingual_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn and apply the transformation\n",
    "transform = learn_transformation(source_matrix, target_matrix)\n",
    "en_dictionary.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44382278145384374\n"
     ]
    }
   ],
   "source": [
    "print(FastVector.cosine_similarity(en_dictionary[\"divide\"], zh_dictionary[\"分裂\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build language classfier to estimate the language effect of word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000000, 300) 4000000\n"
     ]
    }
   ],
   "source": [
    "# build a classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "samples = np.concatenate((en_dictionary.embed, zh_dictionary.embed), axis=0)\n",
    "target = [0] * en_dictionary.embed.shape[0] + [1] * zh_dictionary.embed.shape[0]\n",
    "print(samples.shape, len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()  # default value\n",
    "classifier.fit(samples, target)  # do not return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del samples, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the required input of CLTM\n",
    "1. 100% aligned space\n",
    "2. 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90% aligned space\n",
    "3. check the word types of training input document and word types of aligned space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% aligned space\n",
    "outpath = \"/home/ponshane/work_dir/CLTM-Experiments/Data/Fasttext/100perc-en-zh-wiki-space.txt\"\n",
    "export_to_required_input_of_CLTM(en_dictionary, zh_dictionary, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90% aligned space\n",
    "\n",
    "for perc in [10, 20, 30, 40, 50, 60, 70, 80, 90]:\n",
    "    topRemovedN = (en_dictionary.embed.shape[1] - en_dictionary.embed.shape[1] * (perc/100)) // 2\n",
    "    topRemovedN = int(topRemovedN)\n",
    "    \n",
    "    removal_dimension_list = list(np.argsort(classifier.coef_)[0][:topRemovedN]) + list(np.argsort(classifier.coef_)[0][-topRemovedN:])\n",
    "\n",
    "    en_sub = np.delete(en_dictionary.embed, removal_dimension_list, 1)\n",
    "    zh_sub = np.delete(zh_dictionary.embed, removal_dimension_list, 1)\n",
    "    \n",
    "    outpath = \"/home/ponshane/work_dir/CLTM-Experiments/Data/Fasttext/\" + str(perc) + \"perc-en-zh-wiki-space.txt\"\n",
    "    fout = open(outpath, \"w\")\n",
    "    for token_id in en_dictionary.word2id.values():\n",
    "        vector_components = [\"%.6f\" % number for number in en_sub[token_id,:]]\n",
    "        vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "        out_line = en_dictionary.id2word[token_id] + \" \" + vector_as_string + \"\\n\"\n",
    "        fout.write(out_line)\n",
    "\n",
    "    for token_id in zh_dictionary.word2id.values():\n",
    "        vector_components = [\"%.6f\" % number for number in zh_sub[token_id,:]]\n",
    "        vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "        out_line = zh_dictionary.id2word[token_id] + \" \" + vector_as_string + \"\\n\"\n",
    "        fout.write(out_line)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for MLDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the word types of training input document and word types of aligned space\n",
    "\n",
    "doc_path = \"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/CLTM-MLDoc.txt\"\n",
    "\n",
    "missing_words = set()\n",
    "with open(doc_path, \"r\") as handler:\n",
    "    for line in handler:\n",
    "        for word in line.strip(\"\\n\").split(\" \"):\n",
    "            if word in missing_words:\n",
    "                continue\n",
    "            elif word not in en_dictionary.word2id.keys() and word not in zh_dictionary.word2id.keys():\n",
    "                missing_words.add(word)\n",
    "\n",
    "new_doc_path = \"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/CLTM-MLDoc-fileterd-by-fasttext.txt\"\n",
    "with open(doc_path, \"r\") as handler, open(new_doc_path, \"w\") as newer:\n",
    "    for line in handler:\n",
    "        temp = []\n",
    "        for word in line.strip(\"\\n\").split(\" \"):\n",
    "            if word not in missing_words:\n",
    "                temp.append(word)\n",
    "        one_line = \" \".join(temp) + \"\\n\"\n",
    "        newer.write(one_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4273 1913\n"
     ]
    }
   ],
   "source": [
    "eng_num = 0\n",
    "for word in missing_words:\n",
    "    if isEnglish(word):\n",
    "        eng_num+=1\n",
    "print(len(missing_words), eng_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for UM-Corpus 25K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the word types of training input document and word types of aligned space\n",
    "doc_path = \"/home/ponshane/work_dir/CLTM-Experiments/Data/UM-Corpus/50K-sampled-docs/selected50KDos.txt\"\n",
    "\n",
    "missing_words = set()\n",
    "with open(doc_path, \"r\") as handler:\n",
    "    for line in handler:\n",
    "        for word in line.strip(\"\\n\").split(\" \"):\n",
    "            if word in missing_words:\n",
    "                continue\n",
    "            elif word not in en_dictionary.word2id.keys() and word not in zh_dictionary.word2id.keys():\n",
    "                missing_words.add(word)\n",
    "\n",
    "new_doc_path = \"/home/ponshane/work_dir/CLTM-Experiments/Data/UM-Corpus/50K-sampled-docs/selected50KDos-fileterd-by-fasttext.txt\"\n",
    "with open(doc_path, \"r\") as handler, open(new_doc_path, \"w\") as newer:\n",
    "    for line in handler:\n",
    "        temp = []\n",
    "        for word in line.strip(\"\\n\").split(\" \"):\n",
    "            if word not in missing_words:\n",
    "                temp.append(word)\n",
    "        one_line = \" \".join(temp) + \"\\n\"\n",
    "        newer.write(one_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5583 1810\n"
     ]
    }
   ],
   "source": [
    "eng_num = 0\n",
    "for word in missing_words:\n",
    "    if isEnglish(word):\n",
    "        eng_num+=1\n",
    "print(len(missing_words), eng_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cross-lingual]",
   "language": "python",
   "name": "conda-env-cross-lingual-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
