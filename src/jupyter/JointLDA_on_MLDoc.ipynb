{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from ptm import JointGibbsLDA, JointCorpus\n",
    "from ptm.utils import get_top_words\n",
    "from codebase.topic_evaluator import *\n",
    "from MLDoc_Class_predictor_by_theta import transform_pd_to_numpy_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm=np.linalg.norm(v, ord=1)\n",
    "    if norm==0:\n",
    "        norm=np.finfo(v.dtype).eps\n",
    "    return v/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = pd.read_pickle(\"../out/MLDoc/Shuffled_RS168_tagged_englishAndchinese_corpus_pd.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_idx and target_idx 用來記得 pd_index and corpus idx 的對應\n",
    "source_idx = dict()\n",
    "target_idx = dict()\n",
    "source_file = open(\"../out/JointLDA_Inputs/MLDoc_English.txt\", \"w\")\n",
    "target_file = open(\"../out/JointLDA_Inputs/MLDoc_Chinese.txt\", \"w\")\n",
    "\n",
    "for idx, row in Corpus.iterrows():\n",
    "    if row[\"language\"] == \"English\":\n",
    "        source_idx[idx] = len(source_idx)\n",
    "        source_file.write(\" \".join(row[\"extracted_text\"]) + \"\\n\")\n",
    "    elif row[\"language\"] == \"Chinese\":\n",
    "        target_idx[idx] = len(target_idx)\n",
    "        target_file.write(\" \".join(row[\"extracted_text\"]) + \"\\n\")\n",
    "source_file.close()\n",
    "target_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare corpus\n",
    "corpus = JointCorpus(source_corpus_file=\"../out/JointLDA_Inputs/MLDoc_English.txt\",\n",
    "                     target_corpus_file=\"../out/JointLDA_Inputs/MLDoc_Chinese.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_cn_dict = open(\"/home/ponshane/Downloads/zh-en.txt\", \"r\")\n",
    "f = open(\"../out/JointLDA_Inputs/MLDoc_EN_ZH_dictionaries.csv\", \"w\")\n",
    "for line in en_cn_dict.readlines():\n",
    "    line = line.rstrip(\"\\n\").split(\" \")\n",
    "    target_word = line[0]\n",
    "    source_word = line[1]\n",
    "    #print(source_word, target_word)\n",
    "    if source_word in corpus.source_dict.token2id.keys() and target_word in corpus.target_dict.token2id.keys():\n",
    "        print(source_word, target_word)\n",
    "        f.write(source_word + \",\" + target_word + \"\\n\")\n",
    "en_cn_dict.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-04 16:18:06 INFO:JointCorpus:size of concept: 514, size of source vocab: 11021, size of target vocab: 6688\n",
      "2019-01-04 16:18:06 INFO:JointCorpus:Successfully generate idx corpus 'self.docs' and language flags 'self.language_flags'\n"
     ]
    }
   ],
   "source": [
    "corpus.update_doctionary(\"../out/JointLDA_Inputs/MLDoc_EN_ZH_dictionaries.csv\")\n",
    "corpus.convert_raw_corpus_to_trainable_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "n_topic=10\n",
    "model = JointGibbsLDA(n_doc=len(corpus.docs), n_concept=corpus.n_concept, n_s_vocab=corpus.n_s_vocab,\n",
    "                      n_t_vocab=corpus.n_t_vocab, n_topic=n_topic)\n",
    "model.fit(corpus.docs, corpus.language_flags, max_iter=100)\n",
    "# ~ 11 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = []\n",
    "for idx, row in Corpus.iterrows():\n",
    "    if idx in source_idx.keys():\n",
    "        thetas.append(normalize(model.DT[source_idx[idx], :]))\n",
    "    elif idx in target_idx.keys():\n",
    "        thetas.append(normalize(model.DT[(len(source_idx) + target_idx[idx]), :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import parfit.parfit as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------FITTING MODELS-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0321s.) Setting batch_size=12.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  11 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  11 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  11 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  11 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------SCORING MODELS-------------\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "This model/metric cannot use predict_proba. Using predict for scoring instead.\n",
      "0.15825 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of  11 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "grid = {\n",
    "        'C': [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1e0],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs'],\n",
    "        'multi_class': ['ovr']}\n",
    "\n",
    "paramGrid = ParameterGrid(grid)\n",
    "\n",
    "    \n",
    "Corpus[\"theta\"] = thetas\n",
    "Data_Object, _ = transform_pd_to_numpy_data(Corpus, language=\"Chinese\")\n",
    "bestModel, bestScore, _, _ = pf.bestFit(LogisticRegression, paramGrid,\n",
    "           Data_Object[\"x_train\"], Data_Object[\"y_train\"], Data_Object[\"x_dev\"], Data_Object[\"y_dev\"],\n",
    "           metric = accuracy_score, scoreLabel = \"Accuracy\", showPlot=False)\n",
    "\n",
    "acc = bestModel.score(X=Data_Object[\"x_test\"], y=Data_Object[\"y_test\"])\n",
    "print(acc, bestScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 :  ['中国' 'say' 'cent' '增长' 'price' '企业' 'tonne' ('oil', '石油') '亿元' '报导' '北京'\n",
      " 'trader' '路透社' '日电' '今年' ('month', '月份') '亿美元' '同比' '投资' 'barrel' '发展'\n",
      " '万吨' ('increase', '增加') 'gas' ('week', '星期') '经济' '指出' '人民币' '国家'\n",
      " ('exit', '出口')]\n",
      "Topic 1 :  ['say' 'market' 'trade' ('dollar', '美元') 'trader' 'dealer' '表示' 'close'\n",
      " ('week', '星期') 'day' '路透社' 'point' 'future' '日电' 'yen' 'end' '中国' 'mark'\n",
      " 'price' 'level' ('rise', '上升') 'see' ('fall', '秋天') ('contract', '合同')\n",
      " ('sell', '出售') ('stock', '股票') '美国' 'analyst' 'trading' 'gold']\n",
      "Topic 2 :  ['say' ('corporate', '公司') '香港' '投资' 'police' ('people', '人民') '路透社'\n",
      " ('government', '政府') '日电' 'force' 'official' ('peace', '平安') 'kill'\n",
      " ('city', '城市') ('shanghai', '上海') 'rebel' '集团' '台湾' 'attack' 'report'\n",
      " 'town' '港元' 'tell' 'spokesman' 'troop' 'security' '项目' 'refugee'\n",
      " ('capital', '首都') 'area']\n",
      "Topic 3 :  ['表示' '经济' '路透社' '成长' '央行' '可能' '预期' '日本' '日电' '指出' '美国' '德国'\n",
      " ('rate', '利率') 'win' '认为' ('current', '目前') '分析师' '显示' '维持' '准备' 'say'\n",
      " '经济学家' 'play' '今年' ('year', '年度') '升息' 'game' '会议' 'team' '英国']\n",
      "Topic 4 :  ['say' ('government', '政府') '股市' '指数' 'country' 'tell' '表示' 'meeting' '下跌'\n",
      " '收盘' 'state' 'talk' 'official' ('minister', '大臣') ('policy', '政策') 'meet'\n",
      " '市场' '上涨' '股价指数' 'issue' 'plan' ('year', '年度') ('need', '需要')\n",
      " ('reform', '改革') 'member' '券商' 'conference' 'parliament' '经纪商' 'trade']\n",
      "Topic 5 :  ['percent' ('year', '年度') 'say' ('rise', '上升') 'growth' ('rate', '利率')\n",
      " ('month', '月份') 'inflation' 'quarter' '市场' 'expect' ('increase', '增加')\n",
      " 'price' 'economy' 'export' '成交' 'economist' ('deficit', '赤字') 'figure'\n",
      " 'forecast' '合约' 'report' '交易员' ('fall', '秋天') '今日' 'sale'\n",
      " ('shanghai', '上海') 'grow' '人民币' 'compare']\n",
      "Topic 6 :  [('dollar', '美元') 'say' 'percent' 'bank' ('rate', '利率') 'bond' '期货' '收盘'\n",
      " 'tax' ('interest', '利息') ('year', '年度') '市场' '日电' 'debt' '路透社' 'money'\n",
      " '美分' 'issue' '马克' ('fund', '基金') '交易所' '下跌' '日圆' '公债'\n",
      " ('government', '政府') 'market' '美国' '交易商' '纽约' '表示']\n",
      "Topic 7 :  ['美国' 'say' '英国' '调整' '销售' '去年同期' '指数' '数据' 'party' 'election' '路透社' '物价'\n",
      " '日电' ('year', '年度') '亿美元' '日本' '季节' '公布' '年增率' '第季' '消费者' '货供额' '物价指数'\n",
      " ('rise', '上升') '年率' '工业生产' '订单' ('retailing', '零售') '初值' ('month', '月份')]\n",
      "Topic 8 :  ['share' ('rate', '利率') 'say' 'percent' ('stock', '股票') '央行' '台币' 'market'\n",
      " ('profit', '盈利') 'index' ('rise', '上升') 'point' 'company' '资金' '市场' '今日'\n",
      " '表示' '天期' '银行' 'trade' 'investor' 'price' 'close' 'analyst' 'earning'\n",
      " 'volume' '准备' ('fall', '秋天') '票券' 'broker']\n",
      "Topic 9 :  ['say' ('corporate', '公司') 'company' ('year', '年度') '发行' '万股' '有限公司' '万元'\n",
      " '股份' ('listing', '上市') '每股' 'sale' '日电' 'business' '路透社' 'a股'\n",
      " ('shanghai', '上海') '利润' '香港' 'group' 'plan' ('announcement', '公告')\n",
      " 'market' ('equity', '股本') '表示' 'service' 'b股' '指出' 'expect'\n",
      " ('revenue', '收入')]\n"
     ]
    }
   ],
   "source": [
    "# show top words\n",
    "for ti in range(n_topic):\n",
    "    top_words = get_top_words(model.TW, corpus.reconcatenate_dict, ti, n_words=30)\n",
    "    print('Topic', ti ,': ', top_words)\n",
    "    #print('Topic', ti ,': ', ','.join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.docs[len(source_idx)+114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 745:0, 2068:1, 1171: 95, 5415: 114, 50: 224,\n",
    "#target_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[corpus.target_dict[word] for word in Corpus.loc[5415][\"extracted_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus.loc[50][\"extracted_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_arr = np.empty((10))\n",
    "en_arr = np.empty((10))\n",
    "for idx, row in Corpus.iterrows():\n",
    "    if row[\"language\"] == \"Chinese\":\n",
    "        ch_arr += row[\"theta\"]\n",
    "    elif row[\"language\"] == \"English\":\n",
    "        en_arr += row[\"theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12033252, 0.1129089 , 0.08639831, 0.14099912, 0.07939396,\n",
       "       0.05317105, 0.11453081, 0.12498181, 0.05801409, 0.10943612])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_arr / len(corpus.target_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06346818, 0.11426893, 0.07896215, 0.0539864 , 0.12387346,\n",
       "       0.12879408, 0.12989076, 0.070026  , 0.10865746, 0.12823925])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_arr / len(corpus.source_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cross-lingual]",
   "language": "python",
   "name": "conda-env-cross-lingual-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
