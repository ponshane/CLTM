{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UM-Corpus\n",
    "mainly used for calculate the dictionary coverage and average frequency rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UM_CHINESE_CORPUS = \"/home/ponshane/work_dir/CLTM-Experiments/Data/UM-Corpus/800K-sampled-docs/800K_Chinese_UM_Corpus.txt\"\n",
    "UM_ENGLISH_CORPUS = \"/home/ponshane/work_dir/CLTM-Experiments/Data/UM-Corpus/800K-sampled-docs/800K_English_UM_Corpus.txt\"\n",
    "# UM_DICTIONARY = \"/home/ponshane/work_dir/CLTM-Experiments/Data/UM-Corpus/50K-sampled-docs/um-corpus-news-top-translation.csv\"\n",
    "UM_DICTIONARY = \"/home/ponshane/Downloads/zh-en.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word frequency dictionary\n",
    "UM_Chinese_fdist = FreqDist()\n",
    "with open(UM_CHINESE_CORPUS, \"r\") as handler:\n",
    "    for line in handler:\n",
    "        doc = line.strip(\"\\n\").split(\" \")\n",
    "        for word in doc:\n",
    "            UM_Chinese_fdist[word] +=1\n",
    "\n",
    "            # turn into the word rank list by frequency            \n",
    "UM_Chinese_Freq_rank = sorted(UM_Chinese_fdist , key = UM_Chinese_fdist.__getitem__, reverse = True)\n",
    "UM_Chinese_Freq_rank[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word frequency dictionary\n",
    "UM_English_fdist = FreqDist()\n",
    "with open(UM_ENGLISH_CORPUS, \"r\") as handler:\n",
    "    for line in handler:\n",
    "        doc = line.strip(\"\\n\").split(\" \")\n",
    "        for word in doc:\n",
    "            UM_English_fdist[word] +=1\n",
    "\n",
    "            # turn into the word rank list by frequency            \n",
    "UM_English_Freq_rank = sorted(UM_English_fdist , key = UM_English_fdist.__getitem__, reverse = True)\n",
    "UM_English_Freq_rank[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chinese_set = set(UM_Chinese_Freq_rank)\n",
    "English_set = set(UM_English_Freq_rank)\n",
    "\n",
    "with open(UM_DICTIONARY, \"r\") as handler:\n",
    "    match_rank_list = []\n",
    "    word_pairs = []\n",
    "    for line in handler:\n",
    "        temp = line.strip(\"\\n\").split(\" \")\n",
    "        Chinese_word = temp[0]\n",
    "        English_word = temp[1]\n",
    "        if Chinese_word in Chinese_set and English_word in English_set:\n",
    "            chinese_rank = UM_Chinese_Freq_rank.index(Chinese_word)\n",
    "            english_rank = UM_English_Freq_rank.index(English_word)\n",
    "            word_pairs.append({Chinese_word: English_word})\n",
    "            match_rank_list.append((chinese_rank+english_rank)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2537473284963936"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(match_rank_list[800:3800]) / len(match_rank_list[800:3800])) / max(len(UM_Chinese_Freq_rank), len(UM_English_Freq_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in word_pairs[800:3800]:\n",
    "    for c, e in pair.items():\n",
    "        print(c, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLDoc\n",
    "mainly used for calculate the dictionary coverage and average frequency rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLDoc_CHINESE_CORPUS = \"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/MLDoc-Chinese.txt\"\n",
    "MLDoc_ENGLISH_CORPUS = \"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/MLDoc-English.txt\"\n",
    "MLDoc_DICTIONARY = \"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/MLDoc_EN_ZH_dictionaries.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word frequency dictionary\n",
    "MLDoc_Chinese_fdist = FreqDist()\n",
    "with open(MLDoc_CHINESE_CORPUS, \"r\") as handler:\n",
    "    for line in handler:\n",
    "        doc = line.strip(\"\\n\").split(\" \")\n",
    "        for word in doc:\n",
    "            MLDoc_Chinese_fdist[word] +=1\n",
    "\n",
    "            # turn into the word rank list by frequency            \n",
    "MLDoc_Chinese_Freq_rank = sorted(MLDoc_Chinese_fdist , key = MLDoc_Chinese_fdist.__getitem__, reverse = True)\n",
    "#MLDoc_Chinese_Freq_rank[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word frequency dictionary\n",
    "MLDoc_English_fdist = FreqDist()\n",
    "with open(MLDoc_ENGLISH_CORPUS, \"r\") as handler:\n",
    "    for line in handler:\n",
    "        doc = line.strip(\"\\n\").split(\" \")\n",
    "        for word in doc:\n",
    "            MLDoc_English_fdist[word] +=1\n",
    "\n",
    "            # turn into the word rank list by frequency            \n",
    "MLDoc_English_Freq_rank = sorted(MLDoc_English_fdist , key = MLDoc_English_fdist.__getitem__, reverse = True)\n",
    "#MLDoc_English_Freq_rank[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chinese_set = set(MLDoc_Chinese_Freq_rank)\n",
    "English_set = set(MLDoc_English_Freq_rank)\n",
    "\n",
    "with open(MLDoc_DICTIONARY, \"r\") as handler:\n",
    "    match_rank_list = []\n",
    "    for line in handler:\n",
    "        temp = line.strip(\"\\n\").split(\",\")\n",
    "        Chinese_word = temp[1]\n",
    "        English_word = temp[0]\n",
    "        if Chinese_word in Chinese_set and English_word in English_set:\n",
    "            chinese_rank = MLDoc_Chinese_Freq_rank.index(Chinese_word)\n",
    "            english_rank = MLDoc_English_Freq_rank.index(English_word)\n",
    "            match_rank_list.append((chinese_rank+english_rank)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24077946301811057"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(match_rank_list))\n",
    "(sum(match_rank_list) / len(match_rank_list)) / max(len(MLDoc_Chinese_Freq_rank), len(MLDoc_English_Freq_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-craft dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \",\".join(MLDoc_Chinese_Freq_rank[:])\n",
    "for word in MLDoc_Chinese_Freq_rank[0:4000]:\n",
    "    print(word)\n",
    "    # I paste these words into a file then send it to google translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./zh-trans-all.txt\", \"r\") as zh_handler, open(\"./en-trans-all.txt\", \"r\") as en_handler:\n",
    "    # each line represent a pair\n",
    "    for zh_word, en_word in zip(zh_handler, en_handler):\n",
    "        print(en_word.strip(\"\\n\") + \" \" + zh_word.strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAND_CRAFT_DICTIONARY = \"./hand-craft-4000-dict.txt\"\n",
    "with open(HAND_CRAFT_DICTIONARY, \"r\") as handler:\n",
    "    match_rank_list = []\n",
    "    for line in handler:\n",
    "        temp = line.strip(\"\\n\").split(\" \")\n",
    "        Chinese_word = temp[1]\n",
    "        English_word = temp[0].lower()\n",
    "        if Chinese_word in Chinese_set and English_word in English_set:\n",
    "            chinese_rank = MLDoc_Chinese_Freq_rank.index(Chinese_word)\n",
    "            english_rank = MLDoc_English_Freq_rank.index(English_word)\n",
    "            avg_rank = (chinese_rank+english_rank)/2\n",
    "            if avg_rank < 2100:\n",
    "                match_rank_list.append((chinese_rank+english_rank)/2)\n",
    "                print(English_word + \",\" + Chinese_word)\n",
    "                # we can control and produce our hand-craft dictionary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09955220470124951"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(match_rank_list))\n",
    "(sum(match_rank_list) / len(match_rank_list)) / max(len(MLDoc_Chinese_Freq_rank), len(MLDoc_English_Freq_rank))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cross-lingual]",
   "language": "python",
   "name": "conda-env-cross-lingual-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
