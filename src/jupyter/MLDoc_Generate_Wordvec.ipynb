{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "import sys\n",
    "# set python syspath to point out location of our self-writing module\n",
    "sys.path.append(\"/home/ponshane/work_dir/CLTM/src/codebase/\")\n",
    "\n",
    "from helper import *\n",
    "\n",
    "### init and read config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "MongoDB = config[\"MLDoc\"][\"Database\"]\n",
    "MongoUser = config[\"MLDoc\"][\"User\"]\n",
    "MongoPW = config[\"MLDoc\"][\"PW\"]\n",
    "\n",
    "###連接MONGO\n",
    "uri = \"mongodb://\" + MongoUser + \":\" + MongoPW + \"@140.117.69.70:30241/\" + MongoDB + \"?authMechanism=SCRAM-SHA-1\"\n",
    "\n",
    "client = MongoClient(uri)\n",
    "db = client.MLDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 檢查單字是否都是 英文字/數字 http://hzy3774.iteye.com/blog/2359032\n",
    "def judge_pure_english(keyword):  \n",
    "    return all(ord(c) < 128 for c in keyword)\n",
    "\n",
    "def extract_selected_pos_word(sentence):\n",
    "    # 挑取 #N, #V 詞性開頭的字\n",
    "    indices = [i for i, s in enumerate(sentence) if \"#N\" in s or \"#V\" in s]\n",
    "    select_words = [sentence[index] for index in indices]\n",
    "    \n",
    "    # 清除詞性標籤\n",
    "    words = [re.search('(.*)#', word).group(1) for word in select_words]\n",
    "    \n",
    "    # 過濾單詞\n",
    "    words = [word for word in words if len(word) >= 2]\n",
    "    \n",
    "    # 過濾英文字\n",
    "    words = [word for word in words if judge_pure_english(word) == False]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chinese Documents: 24533\n"
     ]
    }
   ],
   "source": [
    "# convert your date string to datetime object\n",
    "target_collection = db.Chinese\n",
    "num = target_collection.count({\"chi_nlp_process\": {\"$exists\": True}})\n",
    "print(\"Number of Chinese Documents: %d\" % num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "query_documents = target_collection.find({\"chi_nlp_process\": {\"$exists\": True}}, no_cursor_timeout=True)\n",
    "\n",
    "sentence = list()\n",
    "index = 0\n",
    "\n",
    "for each_document in query_documents:\n",
    "    for each_sentence in each_document[\"chi_result\"][\"pos\"]:\n",
    "        sentence.append(extract_selected_pos_word(each_sentence))\n",
    "    \n",
    "    index += 1\n",
    "    if(index % 1000 ==0):\n",
    "        print(\"Already process %d documents\" % index)\n",
    "\n",
    "query_documents.close()\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "# Time elapsed (hh:mm:ss.ms) 0:00:18.737113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['路透社',\n",
       "  '香港',\n",
       "  '中国',\n",
       "  '电子',\n",
       "  '彩管',\n",
       "  '生产商',\n",
       "  '深圳',\n",
       "  '赛格',\n",
       "  '股份',\n",
       "  '有限公司',\n",
       "  '表示',\n",
       "  '今年',\n",
       "  '上半年',\n",
       "  '产量',\n",
       "  '增加',\n",
       "  '预计',\n",
       "  '上半年',\n",
       "  '利润',\n",
       "  '增幅'],\n",
       " ['年报', '显示', '实现', '利润'],\n",
       " ['亿元', '人民币', '中期', '实现', '万元'],\n",
       " ['公司',\n",
       "  '人士',\n",
       "  '香港',\n",
       "  '中国',\n",
       "  '快讯',\n",
       "  '表示',\n",
       "  '赛格',\n",
       "  '公司',\n",
       "  '实施',\n",
       "  '每股',\n",
       "  '送股',\n",
       "  '分红',\n",
       "  '方案',\n",
       "  '上半年',\n",
       "  '利润',\n",
       "  '保持'],\n",
       " []]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "model = gensim.models.Word2Vec(sentence, size=100, window=5,\n",
    "                               min_count=15, workers=4, negative =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('净利润', 0.8554553985595703),\n",
       " ('多万元', 0.8341077566146851),\n",
       " ('盈利', 0.8136324882507324),\n",
       " ('利润总额', 0.7878793478012085),\n",
       " ('收入', 0.7827474474906921),\n",
       " ('销售收入', 0.7672287225723267),\n",
       " ('投资收益', 0.7518516182899475),\n",
       " ('亿万元', 0.745165228843689),\n",
       " ('扭亏', 0.7345239520072937),\n",
       " ('利税', 0.724858283996582)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"利润\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "2018-11-22 10:30:07,962 : INFO : saving Word2Vec object under ../out/MLDoc/MLDoc-chinese-word2vec_NV_s100w5m15n10.vec, separately None\n",
      "2018-11-22 10:30:07,963 : INFO : not storing attribute vectors_norm\n",
      "2018-11-22 10:30:07,964 : INFO : not storing attribute cum_table\n",
      "2018-11-22 10:30:08,015 : INFO : saved ../out/MLDoc/MLDoc-chinese-word2vec_NV_s100w5m15n10.vec\n"
     ]
    }
   ],
   "source": [
    "# expoert to embedding file\n",
    "file_path = \"../out/MLDoc/Chinese_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"../out/MLDoc/Chinese_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write(each_word+\"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "model.save(\"../out/MLDoc/MLDoc-chinese-word2vec_NV_s100w5m15n10.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "target_collection = db.English\n",
    "docs = target_collection.find({\"nlp_process\": {\"$exists\": True}}, no_cursor_timeout=True)\n",
    "\n",
    "index = 0\n",
    "sentence_list = []\n",
    "id_mongo_dictionary = {}\n",
    "\n",
    "for doc in docs:\n",
    "\n",
    "    for each_sentence in doc[\"nested_token_list\"]:\n",
    "        tokens_from_each_sentence = project_function_for_every_document(each_sentence, want_stop=False,\n",
    "                                                                        want_alpha=True, want_lemma=True,\n",
    "                                                                        accept_pos = [\"NOUN\", \"VERB\"],\n",
    "                                                                        use_entity=False)\n",
    "        sentence_list.append(tokens_from_each_sentence)\n",
    "\n",
    "    index += 1\n",
    "    if(index % 5000 ==0):\n",
    "        print(\"Already process %d documents\" % index)\n",
    "        \n",
    "docs.close()\n",
    "\n",
    "time_elapsed = datetime.now() - start_time\n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "# Time elapsed (hh:mm:ss.ms) 0:35:12.122783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9592033"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pickle.dump( sentence_list, open( \"../out/MLDoc/MLDoc_Eng_Sentences.pkl\", \"wb\" ) )\n",
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentence_list, size=100, window=5,\n",
    "                               min_count=15, workers=4, negative =10, sample = 1e-5)\n",
    "# it takes about 4min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crude', 0.7762783765792847),\n",
       " ('gas', 0.7549433708190918),\n",
       " ('barrel', 0.7484706044197083),\n",
       " ('petroleum', 0.7382102012634277),\n",
       " ('liquefy', 0.7294834852218628),\n",
       " ('kilolitre', 0.7153576612472534),\n",
       " ('refinery', 0.7033140659332275),\n",
       " ('refiner', 0.6968631744384766),\n",
       " ('oilfield', 0.6891987919807434),\n",
       " ('mdo', 0.678256630897522)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"oil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "2018-11-22 18:53:22,487 : INFO : saving Word2Vec object under ../out/MLDoc/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.vec, separately None\n",
      "2018-11-22 18:53:22,487 : INFO : not storing attribute vectors_norm\n",
      "2018-11-22 18:53:22,488 : INFO : not storing attribute cum_table\n",
      "2018-11-22 18:53:22,657 : INFO : saved ../out/MLDoc/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.vec\n"
     ]
    }
   ],
   "source": [
    "# expoert to embedding file\n",
    "file_path = \"../out/MLDoc/English_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"../out/MLDoc/English_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write(each_word+\"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "model.save(\"../out/MLDoc/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Japanese\n",
    "Notice that we don't use sentence segmentation here for the limitation of Mecab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_selected_pos_word_from_jap(doc):\n",
    "    # 挑取 #N, #V 詞性開頭的字\n",
    "    indices = [i for i, s in enumerate(doc) if \"#名詞\" in s or \"#動詞\" in s]\n",
    "    select_words = [doc[index] for index in indices]\n",
    "    \n",
    "    # 清除詞性標籤\n",
    "    words = [re.search('(.*)#', word).group(1) for word in select_words]\n",
    "    \n",
    "    # 過濾單詞\n",
    "    words = [word for word in words if len(word) >= 2]\n",
    "    \n",
    "    # 過濾英文字\n",
    "    words = [word for word in words if judge_pure_english(word) == False]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chinese Documents: 58599\n"
     ]
    }
   ],
   "source": [
    "# convert your date string to datetime object\n",
    "target_collection = db.Japanese\n",
    "num = target_collection.count({\"jap_nlp_process\": {\"$exists\": True}})\n",
    "print(\"Number of Chinese Documents: %d\" % num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "query_documents = target_collection.find({\"jap_nlp_process\": {\"$exists\": True}}, no_cursor_timeout=True)\n",
    "\n",
    "jap_sentences = list()\n",
    "index = 0\n",
    "\n",
    "for each_document in query_documents:\n",
    "    jap_sentences.append(extract_selected_pos_word_from_jap(each_document[\"jap_result\"][\"pos\"]))\n",
    "    \n",
    "    index += 1\n",
    "    if(index % 1000 ==0):\n",
    "        print(\"Already process %d documents\" % index)\n",
    "\n",
    "query_documents.close()\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "# Time elapsed (hh:mm:ss.ms) 0:00:18.737113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jap_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(jap_sentences, size=100, window=5,\n",
    "                               min_count=15, workers=4, negative =10, sample=1e-4)\n",
    "# it takes about 4min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('外債', 0.6510068774223328),\n",
       " ('株式投資', 0.6410746574401855),\n",
       " ('ポートフォリオ', 0.6040748357772827),\n",
       " ('ベンチャーキャピタル', 0.6006470918655396),\n",
       " ('配分', 0.58954918384552),\n",
       " ('魅力', 0.5849771499633789),\n",
       " ('外国', 0.5676727294921875),\n",
       " ('直接投資', 0.5562580823898315),\n",
       " ('シフト', 0.5470646619796753),\n",
       " ('エマージング', 0.5427674055099487)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"投資\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "2018-11-22 19:28:59,836 : INFO : saving Word2Vec object under ../out/MLDoc/MLDoc-japanese-word2vec_NV_s100w5m15n10sam1e-4.vec, separately None\n",
      "2018-11-22 19:28:59,837 : INFO : not storing attribute vectors_norm\n",
      "2018-11-22 19:28:59,837 : INFO : not storing attribute cum_table\n",
      "2018-11-22 19:28:59,918 : INFO : saved ../out/MLDoc/MLDoc-japanese-word2vec_NV_s100w5m15n10sam1e-4.vec\n"
     ]
    }
   ],
   "source": [
    "# expoert to embedding file\n",
    "file_path = \"../out/MLDoc/Japanese_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"../out/MLDoc/Japanese_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write(each_word+\"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "model.save(\"../out/MLDoc/MLDoc-japanese-word2vec_NV_s100w5m15n10sam1e-4.vec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cross-lingual]",
   "language": "python",
   "name": "conda-env-cross-lingual-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
