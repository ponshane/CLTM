{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models import TranslationMatrix, translation_matrix\n",
    "\n",
    "def normalised(mat, axis=-1, order=2):\n",
    "        \"\"\"Utility function to normalise the rows of a numpy array.\"\"\"\n",
    "        norm = np.linalg.norm(\n",
    "            mat, axis=axis, ord=order, keepdims=True)\n",
    "        norm[norm == 0] = 1\n",
    "        return mat / norm\n",
    "    \n",
    "def export_to_KeyedVectors(transformed_source_model, target_model, file_path):\n",
    "        \n",
    "        target_model.init_sims()\n",
    "        \n",
    "        print(\"transform_source_model.shape = \", transformed_source_model.mat.shape)\n",
    "        print(\"target_model.shape = \", target_model.wv.vectors_norm.shape)\n",
    "\n",
    "        two_languages_index2word = transformed_source_model.index2word + target_model.wv.index2word\n",
    "        normalised_transform_embedding = normalised(transformed_source_model.mat)\n",
    "        print(\"Normalization is finished!\")\n",
    "        \n",
    "        two_languages_word_vector = np.concatenate((normalised_transform_embedding,\\\n",
    "                                              target_model.wv.vectors_norm), axis=0)\n",
    "        print(\"concatenate_model.shape = \", two_languages_word_vector.shape)\n",
    "\n",
    "        out = open(file_path,'w')\n",
    "        out.write(str(two_languages_word_vector.shape[0]) + \" \" + str(two_languages_word_vector.shape[1]))\n",
    "        out.write(\"\\n\")\n",
    "        for each_word in two_languages_index2word:\n",
    "            out.write(each_word + \" \")\n",
    "            out.write(' '.join(map(str, two_languages_word_vector[two_languages_index2word.index(each_word)])) + \"\\n\")\n",
    "        out.close()\n",
    "\n",
    "        print(\"KeyedVectors have exported to\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English & Chinese Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_vec_file = \"../out/MLDoc/MLDoc-chinese-word2vec_NV_s100w5m15n10.vec\"\n",
    "english_vec_file = \"../out/MLDoc/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.vec\"\n",
    "\n",
    "chinese_model = Word2Vec.load(chinese_vec_file)\n",
    "english_model = Word2Vec.load(english_vec_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/home/ponshane/Downloads/zh-en.txt\", \"r\")\n",
    "\n",
    "word_pairs = []\n",
    "\n",
    "for line in f.readlines():  \n",
    "    line = line.rstrip(\"\\n\").split(\" \")\n",
    "    #print(line[0], line[1].lower())\n",
    "    chinese_word = line[0]\n",
    "    english_word = line[1].lower()\n",
    "    if chinese_word in chinese_model.wv.index2word and english_word in english_model.wv.index2word:\n",
    "        word_pairs.append((chinese_word, english_word))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = TranslationMatrix(chinese_model.wv, english_model.wv)\n",
    "trans_model.train(word_pairs)\n",
    "trans_model.translate([\"增加\", \"政府\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform datatype\n",
    "chinese_model_space = translation_matrix.Space(chinese_model.wv.vectors, index2word=chinese_model.wv.index2word)\n",
    "# transform space\n",
    "transformed_Chinese_model = trans_model.apply_transmat(chinese_model_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_source_model.shape =  (6733, 100)\n",
      "target_model.shape =  (19982, 100)\n",
      "Normalization is finished!\n",
      "concatenate_model.shape =  (26715, 100)\n",
      "KeyedVectors have exported to ../out/MLDoc/Chinese_English_wordvectors.vec\n"
     ]
    }
   ],
   "source": [
    "export_to_KeyedVectors(transformed_Chinese_model, english_model, \"../out/MLDoc/Chinese_English_wordvectors.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test phase\n",
    "Concatenated_model = KeyedVectors.load_word2vec_format(\"../out/MLDoc/Chinese_English_wordvectors.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goverment', 0.8741818070411682),\n",
       " ('cabinet', 0.7273430824279785),\n",
       " ('减税', 0.7260865569114685),\n",
       " ('pledge', 0.7130036354064941),\n",
       " ('parliament', 0.70453280210495),\n",
       " ('minister', 0.7030107975006104),\n",
       " ('overspend', 0.6900051236152649),\n",
       " ('reform', 0.6828949451446533),\n",
       " ('administration', 0.6777451634407043),\n",
       " ('devolve', 0.6767010688781738),\n",
       " ('opposition', 0.6755403280258179),\n",
       " ('urge', 0.6747902631759644),\n",
       " ('克拉克', 0.6732919216156006),\n",
       " ('coalition', 0.6729280948638916),\n",
       " ('财政', 0.6727367043495178),\n",
       " ('预算案', 0.6697919368743896),\n",
       " ('政府', 0.6610081791877747),\n",
       " ('追加预算', 0.6601465940475464),\n",
       " ('dovish', 0.6584186553955078),\n",
       " ('fino', 0.6577595472335815)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Concatenated_model.most_similar(\"government\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel/__main__.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel/__main__.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# export to embedding projector\n",
    "file_path = \"../out/MLDoc/Concatenated_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in Concatenated_model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, Concatenated_model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"../out/MLDoc/Concatenated_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "out.write(\"word\\tlanguage\\n\")\n",
    "for each_word in Concatenated_model.wv.index2word:\n",
    "    if each_word in chinese_model.wv.index2word:\n",
    "        out.write(each_word+\"\\tchinese\\n\")\n",
    "    else:\n",
    "        out.write(each_word+\"\\tenglish\\n\")\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English & Chinese Mapping - Version II (use handcraft dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_vec_file = \"../out/MLDoc/MLDoc-chinese-word2vec_NV_s100w5m15n10.vec\"\n",
    "english_vec_file = \"../out/MLDoc/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.vec\"\n",
    "\n",
    "chinese_model = Word2Vec.load(chinese_vec_file)\n",
    "english_model = Word2Vec.load(english_vec_file)\n",
    "\n",
    "chinese_model.wv.save_word2vec_format(\"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/MLDoc-chinese-word2vec_NV_s100w5m15n10.txt\")\n",
    "english_model.wv.save_word2vec_format(\"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/hand-craft-control-dictionary.txt\", \"r\")\n",
    "\n",
    "word_pairs = []\n",
    "\n",
    "for line in f.readlines():  \n",
    "    line = line.rstrip(\"\\n\").split(\",\")\n",
    "    #print(line[0], line[1].lower())\n",
    "    chinese_word = line[1]\n",
    "    english_word = line[0].lower()\n",
    "    if chinese_word in chinese_model.wv.index2word and english_word in english_model.wv.index2word:\n",
    "        word_pairs.append((chinese_word, english_word))\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(len(word_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = TranslationMatrix(chinese_model.wv, english_model.wv)\n",
    "trans_model.train(word_pairs)\n",
    "trans_model.translate([\"增加\", \"政府\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform datatype\n",
    "chinese_model_space = translation_matrix.Space(chinese_model.wv.vectors, index2word=chinese_model.wv.index2word)\n",
    "# transform space\n",
    "transformed_Chinese_model = trans_model.apply_transmat(chinese_model_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_source_model.shape =  (6733, 100)\n",
      "target_model.shape =  (19982, 100)\n",
      "Normalization is finished!\n",
      "concatenate_model.shape =  (26715, 100)\n",
      "KeyedVectors have exported to /home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/Hand_craft_Chinese_English_wordvectors.vec\n"
     ]
    }
   ],
   "source": [
    "export_to_KeyedVectors(transformed_Chinese_model, english_model, \"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/Hand_craft_Chinese_English_wordvectors.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test phase\n",
    "Concatenated_handcraft_model = KeyedVectors.load_word2vec_format(\"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/Hand_craft_Chinese_English_wordvectors.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8889483322831442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('goverment', 0.8741818070411682),\n",
       " ('cabinet', 0.7273430824279785),\n",
       " ('pledge', 0.7130036354064941),\n",
       " ('国会议员', 0.7065216302871704),\n",
       " ('parliament', 0.70453280210495),\n",
       " ('minister', 0.7030107975006104),\n",
       " ('努力', 0.6921780705451965),\n",
       " ('法案', 0.6915416717529297),\n",
       " ('overspend', 0.6900051236152649),\n",
       " ('reform', 0.6828949451446533),\n",
       " ('主张', 0.6807999610900879),\n",
       " ('税制', 0.6798949241638184),\n",
       " ('行政', 0.6795258522033691),\n",
       " ('administration', 0.6777451634407043),\n",
       " ('devolve', 0.6767010688781738),\n",
       " ('opposition', 0.6755403280258179),\n",
       " ('urge', 0.6747902631759644),\n",
       " ('coalition', 0.6729280948638916),\n",
       " ('减税', 0.6719828248023987),\n",
       " ('白宫', 0.666300892829895)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Concatenated_handcraft_model.similarity(\"国会议员\", \"议员\"))\n",
    "Concatenated_handcraft_model.most_similar(\"government\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# export to embedding projector\n",
    "file_path = \"/home/ponshane/Desktop/Concatenated_Handcraft_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in Concatenated_handcraft_model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, Concatenated_handcraft_model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"/home/ponshane/Desktop/Concatenated_Handcraft_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "out.write(\"word\\tlanguage\\n\")\n",
    "for each_word in Concatenated_handcraft_model.wv.index2word:\n",
    "    if each_word in chinese_model.wv.index2word:\n",
    "        out.write(each_word+\"\\tchinese\\n\")\n",
    "    else:\n",
    "        out.write(each_word+\"\\tenglish\\n\")\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Procrustes alignment to align space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "# set python syspath to point out location of our self-writing module\n",
    "sys.path.append(\"/home/ponshane/work_dir/CLTM/src/codebase/\")\n",
    "\n",
    "from fasttext import FastVector\n",
    "\n",
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)\n",
    "\n",
    "def load_bilingual_dict(dictionary_path, reverse_source_target = False, sperator = \" \"):\n",
    "    bilingual_dict = []\n",
    "    with open(dictionary_path, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            temp = line.strip(\"\\n\").split(sperator)\n",
    "            if reverse_source_target == True:\n",
    "                bilingual_dict.append((temp[1], temp[0]))\n",
    "            elif reverse_source_target == False:\n",
    "                bilingual_dict.append((temp[0], temp[1]))\n",
    "        return bilingual_dict\n",
    "\n",
    "def export_to_required_input_of_CLTM(en_dictionary, zh_dictionary, outpath):\n",
    "    fout = open(outpath, \"w\")\n",
    "    \n",
    "    vocab_sizes = en_dictionary.n_words + zh_dictionary.n_words\n",
    "    out_line = str(vocab_sizes) + \" \" + str(en_dictionary.n_dim) + \"\\n\"\n",
    "    fout.write(out_line)\n",
    "    \n",
    "    for token in en_dictionary.id2word:\n",
    "        vector_components = [\"%.6f\" % number for number in en_dictionary[token]]\n",
    "        vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "        out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "        fout.write(out_line)\n",
    "\n",
    "    for token in zh_dictionary.id2word:\n",
    "        vector_components = [\"%.6f\" % number for number in zh_dictionary[token]]\n",
    "        vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "        out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "        fout.write(out_line)\n",
    "    fout.close()\n",
    "    \n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from /home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/MLDoc-chinese-word2vec_NV_s100w5m15n10.txt\n",
      "reading word vectors from /home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.txt\n",
      "-0.18624678268317557\n",
      "0.6779499719647438\n"
     ]
    }
   ],
   "source": [
    "# load back the space\n",
    "zh_dictionary = FastVector(vector_file='/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/MLDoc-chinese-word2vec_NV_s100w5m15n10.txt')\n",
    "en_dictionary = FastVector(vector_file='/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.txt')\n",
    "\n",
    "print(FastVector.cosine_similarity(en_dictionary[\"govenment\"], zh_dictionary[\"政党\"]))\n",
    "\n",
    "# load back dictionary\n",
    "bilingual_dict = load_bilingual_dict(dictionary_path=\"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/hand-craft-control-dictionary.txt\",\n",
    "                                     reverse_source_target= True, sperator=\",\")\n",
    "\n",
    "# form the training matrices\n",
    "source_matrix, target_matrix = make_training_matrices(\n",
    "    zh_dictionary, en_dictionary, bilingual_dict)\n",
    "\n",
    "# learn and apply the transformation\n",
    "transform = learn_transformation(source_matrix, target_matrix)\n",
    "zh_dictionary.apply_transform(transform)\n",
    "\n",
    "print(FastVector.cosine_similarity(en_dictionary[\"govenment\"], zh_dictionary[\"政党\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% aligned space\n",
    "outpath = \"/home/ponshane/work_dir/CLTM-Experiments/Data/MLDoc/word-vectors/Procrustes_Hand_craft_Chinese_English_wordvectors.vec\"\n",
    "export_to_required_input_of_CLTM(en_dictionary, zh_dictionary, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concatenated_Procrustes_handcraft_model = KeyedVectors.load_word2vec_format(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9045977079702342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('goverment', 0.874181866645813),\n",
       " ('cabinet', 0.7273430824279785),\n",
       " ('国会议员', 0.7221354842185974),\n",
       " ('减税', 0.7218571901321411),\n",
       " ('pledge', 0.7130035758018494),\n",
       " ('parliament', 0.7045326828956604),\n",
       " ('minister', 0.7030106782913208),\n",
       " ('overspend', 0.6900051832199097),\n",
       " ('歧见', 0.6886298656463623),\n",
       " ('reform', 0.6828948259353638),\n",
       " ('主张', 0.6817994117736816),\n",
       " ('administration', 0.6777451038360596),\n",
       " ('devolve', 0.6767009496688843),\n",
       " ('税制', 0.6764950752258301),\n",
       " ('opposition', 0.6755402088165283),\n",
       " ('urge', 0.6747902035713196),\n",
       " ('coalition', 0.672927975654602),\n",
       " ('援助', 0.6701107025146484),\n",
       " ('会面', 0.6621206998825073),\n",
       " ('议员', 0.6615113019943237)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Concatenated_Procrustes_handcraft_model.similarity(\"国会议员\", \"议员\"))\n",
    "Concatenated_Procrustes_handcraft_model.most_similar(\"government\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# export to embedding projector\n",
    "file_path = \"/home/ponshane/Desktop/Concatenated_Procrustes_Handcraft_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in Concatenated_Procrustes_handcraft_model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, Concatenated_Procrustes_handcraft_model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"/home/ponshane/Desktop/Concatenated_Procrustes_Handcraft_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "out.write(\"word\\tlanguage\\n\")\n",
    "for each_word in Concatenated_Procrustes_handcraft_model.wv.index2word:\n",
    "    if each_word in zh_dictionary.id2word:\n",
    "        out.write(each_word+\"\\tchinese\\n\")\n",
    "    else:\n",
    "        out.write(each_word+\"\\tenglish\\n\")\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English & Japanesne Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_vec_file = \"../out/MLDoc/MLDoc-japanese-word2vec_NV_s100w5m15n10sam1e-4.vec\"\n",
    "english_vec_file = \"../out/MLDoc/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.vec\"\n",
    "\n",
    "japanese_model = Word2Vec.load(japanese_vec_file)\n",
    "english_model = Word2Vec.load(english_vec_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/home/ponshane/Downloads/ja-en.txt\", \"r\")\n",
    "\n",
    "word_pairs = []\n",
    "\n",
    "for line in f.readlines():\n",
    "    line = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "    #print(line[0], line[1].lower())\n",
    "    japanese_word = line[0]\n",
    "    english_word = line[1].lower()\n",
    "    if japanese_word in japanese_model.wv.index2word and english_word in english_model.wv.index2word:\n",
    "        word_pairs.append((japanese_word, english_word))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('終了', 'quit'),\n",
       " ('終了', 'finish'),\n",
       " ('終了', 'end'),\n",
       " ('終了', 'exit'),\n",
       " ('中心', 'center'),\n",
       " ('中心', 'centre'),\n",
       " ('内容', 'content'),\n",
       " ('文化', 'culture'),\n",
       " ('イギリス', 'england'),\n",
       " ('中国', 'china'),\n",
       " ('指定', 'designation'),\n",
       " ('指定', 'specify'),\n",
       " ('位置', 'position'),\n",
       " ('人物', 'person'),\n",
       " ('公開', 'public'),\n",
       " ('公開', 'publish'),\n",
       " ('最終', 'last'),\n",
       " ('最終', 'final'),\n",
       " ('確認', 'confirmation'),\n",
       " ('確認', 'confirm'),\n",
       " ('一般', 'general'),\n",
       " ('歴史', 'history'),\n",
       " ('選挙', 'election'),\n",
       " ('計画', 'planning'),\n",
       " ('計画', 'plan')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(word_pairs))\n",
    "word_pairs[100:125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = TranslationMatrix(japanese_model.wv, english_model.wv)\n",
    "trans_model.train(word_pairs)\n",
    "#trans_model.translate([\"存在\", \"研究\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform datatype\n",
    "japanese_model_space = translation_matrix.Space(japanese_model.wv.vectors, index2word=japanese_model.wv.index2word)\n",
    "# transform space\n",
    "transformed_Japanese_model = trans_model.apply_transmat(japanese_model_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_source_model.shape =  (12894, 100)\n",
      "target_model.shape =  (19982, 100)\n",
      "Normalization is finished!\n",
      "concatenate_model.shape =  (32876, 100)\n",
      "KeyedVectors have exported to ../out/MLDoc/Japanese_English_wordvectors.vec\n"
     ]
    }
   ],
   "source": [
    "export_to_KeyedVectors(transformed_Japanese_model, english_model, \"../out/MLDoc/Japanese_English_wordvectors.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concatenated_model = KeyedVectors.load_word2vec_format(\"../out/MLDoc/Japanese_English_wordvectors.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel/__main__.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel/__main__.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../out/MLDoc/Concatenated_ja-en_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in Concatenated_model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, Concatenated_model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"../out/MLDoc/Concatenated_ja-en_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "out.write(\"word\\tlanguage\\n\")\n",
    "for each_word in Concatenated_model.wv.index2word:\n",
    "    if each_word in japanese_model.wv.index2word:\n",
    "        out.write(each_word+\"\\tjapanese\\n\")\n",
    "    else:\n",
    "        out.write(each_word+\"\\tenglish\\n\")\n",
    "\n",
    "out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cross-lingual]",
   "language": "python",
   "name": "conda-env-cross-lingual-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
