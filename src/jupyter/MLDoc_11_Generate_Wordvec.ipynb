{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "import sys\n",
    "# set python syspath to point out location of our self-writing module\n",
    "sys.path.append(\"/home/ponshane/work_dir/CLTM/src/codebase/\")\n",
    "\n",
    "from helper import *\n",
    "\n",
    "### init and read config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "MongoDB = config[\"MLDoc\"][\"Database\"]\n",
    "MongoUser = config[\"MLDoc\"][\"User\"]\n",
    "MongoPW = config[\"MLDoc\"][\"PW\"]\n",
    "\n",
    "###連接MONGO\n",
    "uri = \"mongodb://\" + MongoUser + \":\" + MongoPW + \"@140.117.69.70:30241/\" + MongoDB + \"?authMechanism=SCRAM-SHA-1\"\n",
    "\n",
    "client = MongoClient(uri)\n",
    "db = client.MLDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 檢查單字是否都是 英文字/數字 http://hzy3774.iteye.com/blog/2359032\n",
    "def judge_pure_english(keyword):  \n",
    "    return all(ord(c) < 128 for c in keyword)\n",
    "\n",
    "def extract_selected_pos_word(sentence):\n",
    "    # 挑取 #N, #V 詞性開頭的字\n",
    "    indices = [i for i, s in enumerate(sentence) if \"#N\" in s or \"#V\" in s]\n",
    "    select_words = [sentence[index] for index in indices]\n",
    "    \n",
    "    # 清除詞性標籤\n",
    "    words = [re.search('(.*)#', word).group(1) for word in select_words]\n",
    "    \n",
    "    # 過濾單詞\n",
    "    words = [word for word in words if len(word) >= 2]\n",
    "    \n",
    "    # 過濾英文字\n",
    "    words = [word for word in words if judge_pure_english(word) == False]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chinese Documents: 24533\n"
     ]
    }
   ],
   "source": [
    "# convert your date string to datetime object\n",
    "target_collection = db.Chinese\n",
    "num = target_collection.count({\"chi_nlp_process\": {\"$exists\": True}})\n",
    "print(\"Number of Chinese Documents: %d\" % num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "query_documents = target_collection.find({\"chi_nlp_process\": {\"$exists\": True}}, no_cursor_timeout=True)\n",
    "\n",
    "sentence = list()\n",
    "index = 0\n",
    "\n",
    "for each_document in query_documents:\n",
    "    for each_sentence in each_document[\"chi_result\"][\"pos\"]:\n",
    "        sentence.append(extract_selected_pos_word(each_sentence))\n",
    "    \n",
    "    index += 1\n",
    "    if(index % 1000 ==0):\n",
    "        print(\"Already process %d documents\" % index)\n",
    "\n",
    "query_documents.close()\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "# Time elapsed (hh:mm:ss.ms) 0:00:18.737113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['路透社',\n",
       "  '香港',\n",
       "  '中国',\n",
       "  '电子',\n",
       "  '彩管',\n",
       "  '生产商',\n",
       "  '深圳',\n",
       "  '赛格',\n",
       "  '股份',\n",
       "  '有限公司',\n",
       "  '表示',\n",
       "  '今年',\n",
       "  '上半年',\n",
       "  '产量',\n",
       "  '增加',\n",
       "  '预计',\n",
       "  '上半年',\n",
       "  '利润',\n",
       "  '增幅'],\n",
       " ['年报', '显示', '实现', '利润'],\n",
       " ['亿元', '人民币', '中期', '实现', '万元'],\n",
       " ['公司',\n",
       "  '人士',\n",
       "  '香港',\n",
       "  '中国',\n",
       "  '快讯',\n",
       "  '表示',\n",
       "  '赛格',\n",
       "  '公司',\n",
       "  '实施',\n",
       "  '每股',\n",
       "  '送股',\n",
       "  '分红',\n",
       "  '方案',\n",
       "  '上半年',\n",
       "  '利润',\n",
       "  '保持'],\n",
       " []]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "model = gensim.models.Word2Vec(sentence, size=100, window=5,\n",
    "                               min_count=15, workers=4, negative =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('净利润', 0.8554553985595703),\n",
       " ('多万元', 0.8341077566146851),\n",
       " ('盈利', 0.8136324882507324),\n",
       " ('利润总额', 0.7878793478012085),\n",
       " ('收入', 0.7827474474906921),\n",
       " ('销售收入', 0.7672287225723267),\n",
       " ('投资收益', 0.7518516182899475),\n",
       " ('亿万元', 0.745165228843689),\n",
       " ('扭亏', 0.7345239520072937),\n",
       " ('利税', 0.724858283996582)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"利润\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "2018-11-22 10:30:07,962 : INFO : saving Word2Vec object under ../out/MLDoc/MLDoc-chinese-word2vec_NV_s100w5m15n10.vec, separately None\n",
      "2018-11-22 10:30:07,963 : INFO : not storing attribute vectors_norm\n",
      "2018-11-22 10:30:07,964 : INFO : not storing attribute cum_table\n",
      "2018-11-22 10:30:08,015 : INFO : saved ../out/MLDoc/MLDoc-chinese-word2vec_NV_s100w5m15n10.vec\n"
     ]
    }
   ],
   "source": [
    "# expoert to embedding file\n",
    "file_path = \"../out/MLDoc/Chinese_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"../out/MLDoc/Chinese_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write(each_word+\"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "model.save(\"../out/MLDoc/MLDoc-chinese-word2vec_NV_s100w5m15n10.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "target_collection = db.English\n",
    "docs = target_collection.find({\"nlp_process\": {\"$exists\": True}}, no_cursor_timeout=True)\n",
    "\n",
    "index = 0\n",
    "sentence_list = []\n",
    "id_mongo_dictionary = {}\n",
    "\n",
    "for doc in docs:\n",
    "\n",
    "    for each_sentence in doc[\"nested_token_list\"]:\n",
    "        tokens_from_each_sentence = project_function_for_every_document(each_sentence, want_stop=False,\n",
    "                                                                        want_alpha=True, want_lemma=True,\n",
    "                                                                        accept_pos = [\"NOUN\", \"VERB\"],\n",
    "                                                                        use_entity=False)\n",
    "        sentence_list.append(tokens_from_each_sentence)\n",
    "\n",
    "    index += 1\n",
    "    if(index % 5000 ==0):\n",
    "        print(\"Already process %d documents\" % index)\n",
    "        \n",
    "docs.close()\n",
    "\n",
    "time_elapsed = datetime.now() - start_time\n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "# Time elapsed (hh:mm:ss.ms) 0:15:07.259858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9592033"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle.dump( sentence_list, open( \"../out/MLDoc/MLDoc_Eng_Sentences.pkl\", \"wb\" ) )\n",
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentence_list, size=100, window=5,\n",
    "                               min_count=15, workers=4, negative =10, sample = 1e-5)\n",
    "# it takes about 4min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-26 04:45:59,229 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('crude', 0.7671653628349304),\n",
       " ('petroleum', 0.750298261642456),\n",
       " ('gas', 0.7449700832366943),\n",
       " ('barrel', 0.7310149073600769),\n",
       " ('liquefy', 0.715983510017395),\n",
       " ('kilolitre', 0.7007793188095093),\n",
       " ('refinery', 0.6914912462234497),\n",
       " ('refiner', 0.6849839687347412),\n",
       " ('bpd', 0.679957389831543),\n",
       " ('heating', 0.6708714962005615)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"oil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "2020-08-26 04:46:17,152 : INFO : saving Word2Vec object under ../out/MLDoc/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.vec, separately None\n",
      "2020-08-26 04:46:17,152 : INFO : not storing attribute vectors_norm\n",
      "2020-08-26 04:46:17,153 : INFO : not storing attribute cum_table\n",
      "2020-08-26 04:46:17,287 : INFO : saved ../out/MLDoc/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.vec\n"
     ]
    }
   ],
   "source": [
    "# expoert to embedding file\n",
    "file_path = \"../out/MLDoc/English_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"../out/MLDoc/English_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write(each_word+\"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "model.save(\"../out/MLDoc/MLDoc-english-word2vec_NV_s100w5m15n10sam1e-5.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Japanese\n",
    "Notice that we don't use sentence segmentation here for the limitation of Mecab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_selected_pos_word_from_jap(doc):\n",
    "    # 挑取 #N, #V 詞性開頭的字\n",
    "    indices = [i for i, s in enumerate(doc) if \"#名詞\" in s or \"#動詞\" in s]\n",
    "    select_words = [doc[index] for index in indices]\n",
    "    \n",
    "    # 清除詞性標籤\n",
    "    words = [re.search('(.*)#', word).group(1) for word in select_words]\n",
    "    \n",
    "    # 過濾單詞\n",
    "    words = [word for word in words if len(word) >= 2]\n",
    "    \n",
    "    # 過濾英文字\n",
    "    words = [word for word in words if judge_pure_english(word) == False]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chinese Documents: 58599\n"
     ]
    }
   ],
   "source": [
    "# convert your date string to datetime object\n",
    "target_collection = db.Japanese\n",
    "num = target_collection.count_documents({\"jap_nlp_process\": {\"$exists\": True}})\n",
    "print(\"Number of Chinese Documents: %d\" % num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already process 1000 documents\n",
      "Already process 2000 documents\n",
      "Already process 3000 documents\n",
      "Already process 4000 documents\n",
      "Already process 5000 documents\n",
      "Already process 6000 documents\n",
      "Already process 7000 documents\n",
      "Already process 8000 documents\n",
      "Already process 9000 documents\n",
      "Already process 10000 documents\n",
      "Already process 11000 documents\n",
      "Already process 12000 documents\n",
      "Already process 13000 documents\n",
      "Already process 14000 documents\n",
      "Already process 15000 documents\n",
      "Already process 16000 documents\n",
      "Already process 17000 documents\n",
      "Already process 18000 documents\n",
      "Already process 19000 documents\n",
      "Already process 20000 documents\n",
      "Already process 21000 documents\n",
      "Already process 22000 documents\n",
      "Already process 23000 documents\n",
      "Already process 24000 documents\n",
      "Already process 25000 documents\n",
      "Already process 26000 documents\n",
      "Already process 27000 documents\n",
      "Already process 28000 documents\n",
      "Already process 29000 documents\n",
      "Already process 30000 documents\n",
      "Already process 31000 documents\n",
      "Already process 32000 documents\n",
      "Already process 33000 documents\n",
      "Already process 34000 documents\n",
      "Already process 35000 documents\n",
      "Already process 36000 documents\n",
      "Already process 37000 documents\n",
      "Already process 38000 documents\n",
      "Already process 39000 documents\n",
      "Already process 40000 documents\n",
      "Already process 41000 documents\n",
      "Already process 42000 documents\n",
      "Already process 43000 documents\n",
      "Already process 44000 documents\n",
      "Already process 45000 documents\n",
      "Already process 46000 documents\n",
      "Already process 47000 documents\n",
      "Already process 48000 documents\n",
      "Already process 49000 documents\n",
      "Already process 50000 documents\n",
      "Already process 51000 documents\n",
      "Already process 52000 documents\n",
      "Already process 53000 documents\n",
      "Already process 54000 documents\n",
      "Already process 55000 documents\n",
      "Already process 56000 documents\n",
      "Already process 57000 documents\n",
      "Already process 58000 documents\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:18.324362\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "query_documents = target_collection.find({\"jap_nlp_process\": {\"$exists\": True}}, no_cursor_timeout=True)\n",
    "\n",
    "jap_sentences = list()\n",
    "index = 0\n",
    "\n",
    "for each_document in query_documents:\n",
    "    jap_sentences.append(extract_selected_pos_word_from_jap(each_document[\"jap_result\"][\"pos\"]))\n",
    "    \n",
    "    index += 1\n",
    "    if(index % 1000 ==0):\n",
    "        print(\"Already process %d documents\" % index)\n",
    "\n",
    "query_documents.close()\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "# Time elapsed (hh:mm:ss.ms) 0:00:18.737113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jap_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-26 04:06:22,295 : INFO : collecting all words and their counts\n",
      "2020-08-26 04:06:22,296 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-08-26 04:06:22,381 : INFO : PROGRESS: at sentence #10000, processed 911461 words, keeping 28743 word types\n",
      "2020-08-26 04:06:22,465 : INFO : PROGRESS: at sentence #20000, processed 1818927 words, keeping 39075 word types\n",
      "2020-08-26 04:06:22,552 : INFO : PROGRESS: at sentence #30000, processed 2723166 words, keeping 47590 word types\n",
      "2020-08-26 04:06:22,639 : INFO : PROGRESS: at sentence #40000, processed 3624877 words, keeping 53701 word types\n",
      "2020-08-26 04:06:22,723 : INFO : PROGRESS: at sentence #50000, processed 4505889 words, keeping 58801 word types\n",
      "2020-08-26 04:06:22,798 : INFO : collected 63302 word types from a corpus of 5277657 raw words and 58599 sentences\n",
      "2020-08-26 04:06:22,798 : INFO : Loading a fresh vocabulary\n",
      "2020-08-26 04:06:22,930 : INFO : min_count=15 retains 12894 unique words (20% of original 63302, drops 50408)\n",
      "2020-08-26 04:06:22,931 : INFO : min_count=15 leaves 5126284 word corpus (97% of original 5277657, drops 151373)\n",
      "2020-08-26 04:06:22,951 : INFO : deleting the raw counts dictionary of 63302 items\n",
      "2020-08-26 04:06:22,952 : INFO : sample=0.0001 downsamples 662 most-common words\n",
      "2020-08-26 04:06:22,952 : INFO : downsampling leaves estimated 2961245 word corpus (57.8% of prior 5126284)\n",
      "2020-08-26 04:06:22,971 : INFO : estimated required memory for 12894 words and 100 dimensions: 16762200 bytes\n",
      "2020-08-26 04:06:22,972 : INFO : resetting layer weights\n",
      "2020-08-26 04:06:23,087 : INFO : training model with 4 workers on 12894 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=10 window=5\n",
      "2020-08-26 04:06:24,095 : INFO : EPOCH 1 - PROGRESS: at 56.47% examples, 1669512 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-26 04:06:24,813 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-26 04:06:24,818 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-26 04:06:24,821 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-26 04:06:24,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-26 04:06:24,823 : INFO : EPOCH - 1 : training on 5277657 raw words (2961201 effective words) took 1.7s, 1707809 effective words/s\n",
      "2020-08-26 04:06:25,825 : INFO : EPOCH 2 - PROGRESS: at 57.73% examples, 1718103 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-26 04:06:26,526 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-26 04:06:26,528 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-26 04:06:26,531 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-26 04:06:26,533 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-26 04:06:26,533 : INFO : EPOCH - 2 : training on 5277657 raw words (2961116 effective words) took 1.7s, 1733258 effective words/s\n",
      "2020-08-26 04:06:27,539 : INFO : EPOCH 3 - PROGRESS: at 58.25% examples, 1731976 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-26 04:06:28,229 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-26 04:06:28,235 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-26 04:06:28,238 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-26 04:06:28,239 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-26 04:06:28,240 : INFO : EPOCH - 3 : training on 5277657 raw words (2961956 effective words) took 1.7s, 1738777 effective words/s\n",
      "2020-08-26 04:06:29,246 : INFO : EPOCH 4 - PROGRESS: at 58.41% examples, 1734891 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-26 04:06:29,925 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-26 04:06:29,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-26 04:06:29,929 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-26 04:06:29,934 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-26 04:06:29,934 : INFO : EPOCH - 4 : training on 5277657 raw words (2961518 effective words) took 1.7s, 1750328 effective words/s\n",
      "2020-08-26 04:06:30,940 : INFO : EPOCH 5 - PROGRESS: at 57.59% examples, 1706023 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-26 04:06:31,640 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-26 04:06:31,643 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-26 04:06:31,645 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-26 04:06:31,648 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-26 04:06:31,649 : INFO : EPOCH - 5 : training on 5277657 raw words (2960956 effective words) took 1.7s, 1729639 effective words/s\n",
      "2020-08-26 04:06:31,649 : INFO : training on a 26388285 raw words (14806747 effective words) took 8.6s, 1729369 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(jap_sentences, size=100, window=5,\n",
    "                               min_count=15, workers=4, negative =10, sample=1e-4)\n",
    "# it takes about 4min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('原油', 0.7877740859985352),\n",
       " ('石油製品', 0.7258394956588745),\n",
       " ('原油価格', 0.7218372225761414),\n",
       " ('オイル', 0.7171489596366882),\n",
       " ('ガス', 0.7155888080596924),\n",
       " ('ナイジェリア', 0.7009726762771606),\n",
       " ('天然ガス', 0.6996159553527832),\n",
       " ('産油', 0.6895568370819092),\n",
       " ('イラク', 0.6825350522994995),\n",
       " ('産油国', 0.6799705028533936)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"石油\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponshane/anaconda3/envs/cross-lingual/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "2020-08-26 04:09:17,544 : INFO : saving Word2Vec object under ../out/MLDoc/MLDoc-japanese-word2vec_NV_s100w5m15n10sam1e-4.vec, separately None\n",
      "2020-08-26 04:09:17,544 : INFO : not storing attribute vectors_norm\n",
      "2020-08-26 04:09:17,545 : INFO : not storing attribute cum_table\n",
      "2020-08-26 04:09:17,635 : INFO : saved ../out/MLDoc/MLDoc-japanese-word2vec_NV_s100w5m15n10sam1e-4.vec\n"
     ]
    }
   ],
   "source": [
    "# expoert to embedding file\n",
    "file_path = \"../out/MLDoc/Japanese_embeddings.tsv\"\n",
    "out = open(file_path,'w')\n",
    "\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write('\\t'.join(map(str, model[each_word])) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "# expoert to metadata file\n",
    "file_path = \"../out/MLDoc/Japanese_metadata.tsv\"\n",
    "out = open(file_path,'w')\n",
    "for each_word in model.wv.index2word:\n",
    "    out.write(each_word+\"\\n\")\n",
    "\n",
    "out.close()\n",
    "\n",
    "model.save(\"../out/MLDoc/MLDoc-japanese-word2vec_NV_s100w5m15n10sam1e-4.vec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-lingual",
   "language": "python",
   "name": "cross-lingual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
